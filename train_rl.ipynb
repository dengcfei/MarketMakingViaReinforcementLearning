{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d69c75-1eec-43ab-9bd1-ac7c1213ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime, time\n",
    "import time as time_module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52a393e5-8590-44b5-9949-319d2257df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_max_norm(data):\n",
    "    minVals = data.min()\n",
    "    maxVals = data.max()\n",
    "    ranges = maxVals - minVals\n",
    "    if ranges==0:\n",
    "        return data*0\n",
    "    normData = data - minVals\n",
    "    normData = normData/ranges\n",
    "    return normData\n",
    "\n",
    "def z_norm(data):\n",
    "    return (data-data.mean())/(data.std()+1e-7)\n",
    "\n",
    "def lob_norm(data_, midprice):\n",
    "    data = data_.copy()\n",
    "    for i in range(10):\n",
    "        data[f'ask_price_{i+1}'] = data[f'ask_price_{i+1}']/(midprice+1e-7) - 1\n",
    "        data[f'bid_price_{i+1}'] = data[f'bid_price_{i+1}']/(midprice+1e-7) - 1\n",
    "        # data[f'ask{i+1}_price'] = z_norm(data[f'ask{i+1}_price'])\n",
    "        # data[f'bid{i+1}_price'] = z_norm(data[f'bid{i+1}_price'])\n",
    "        data[f'ask_quantity_{i+1}'] = data[f'ask_quantity_{i+1}']/data[f'ask_quantity_{i+1}'].max()\n",
    "        data[f'bid_quantity_{i+1}'] = data[f'bid_quantity_{i+1}']/data[f'bid_quantity_{i+1}'].max()\n",
    "\n",
    "    return data\n",
    "\n",
    "def onehot_label(targets):\n",
    "    from tensorflow import keras\n",
    "    # targets: pd.DataFrame len(data)*n_horizons\n",
    "    all_label = []\n",
    "    for i in range(targets.shape[1]):\n",
    "        label = targets.iloc[:,i] - 1\n",
    "        label = keras.utils.to_categorical(label, 3)\n",
    "        # label = label.reshape(len(label), 1, 3)\n",
    "        all_label.append(label)\n",
    "    return np.hstack(all_label)\n",
    "\n",
    "def day2date(day):\n",
    "    day = list(day)\n",
    "    day.insert(4,'-')\n",
    "    day.insert(7,'-')\n",
    "    date = ''.join(day)\n",
    "    return date\n",
    "\n",
    "def pd_is_equal(state_1, state_2):\n",
    "    tmp_1 = state_1.iloc[:,1:]\n",
    "    tmp_2 = state_2.iloc[:,1:]\n",
    "    return tmp_1.equals(tmp_2)\n",
    "\n",
    "def load_data(code, datelist, horizon=10):\n",
    "    if type(datelist) is str:\n",
    "        datelist = [datelist]\n",
    "    data_list = []\n",
    "    for day in datelist:\n",
    "        #ask = pd.read_csv(f\"data/{code}/{day}/ask.csv\")\n",
    "        #bid = pd.read_csv(f\"data/{code}/{day}/bid.csv\").drop(['timestamp'], axis = 1)\n",
    "        #price = pd.read_csv(f\"data/{code}/{day}/price.csv\").drop(['timestamp', 'ask1_price', 'bid1_price'], axis = 1)\n",
    "        #data = pd.concat([ask, bid, price], axis=1)\n",
    "        price = pd.read_csv(f\"data/{code}/{day}/{price_file}\")\n",
    "        data['date'] = data['timestamp'].str.split(expand=True)[0]\n",
    "        data['time'] = data['timestamp'].str.split(expand=True)[1]\n",
    "        data.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "        data['y']=getLabel(data.midprice, horizon)\n",
    "\n",
    "        data_list.append(data)\n",
    "    return pd.concat(data_list)\n",
    "\n",
    "def getLabel(mid_price, horizon, threshold=1e-5):\n",
    "    price_past = mid_price.rolling(window=horizon).mean()\n",
    "\n",
    "    price_future = mid_price.copy()\n",
    "    price_future[:-horizon] = price_past[horizon:]\n",
    "    price_future[-horizon:] = np.nan\n",
    "\n",
    "    pct_change = (price_future - price_past)/price_past\n",
    "    pct_change[pct_change>=threshold] = 1\n",
    "    pct_change[(pct_change<threshold) & (-threshold<pct_change)] = 2\n",
    "    pct_change[pct_change<=-threshold] = 3\n",
    "    return pct_change\n",
    "\n",
    "def process_data(data):\n",
    "    data = data[(data.time > '10:00:00')&(data.time < '14:30:00')]\n",
    "    data = data.dropna()\n",
    "    data.y = data.y.astype(int)\n",
    "\n",
    "    for i in range(10):\n",
    "        data[f'ask{i+1}_price'] = data[f'ask{i+1}_price']/data['midprice'] - 1\n",
    "        data[f'bid{i+1}_price'] = data[f'bid{i+1}_price']/data['midprice'] - 1\n",
    "        # data[f'ask{i+1}_price'] = z_norm(data[f'ask{i+1}_price'])\n",
    "        # data[f'bid{i+1}_price'] = z_norm(data[f'bid{i+1}_price'])\n",
    "        data[f'ask{i+1}_volume'] = data[f'ask{i+1}_volume']/data[f'ask{i+1}_volume'].max()\n",
    "        data[f'bid{i+1}_volume'] = data[f'bid{i+1}_volume']/data[f'bid{i+1}_volume'].max()\n",
    "\n",
    "    return data.set_index(['date', 'time'])\n",
    "\n",
    "def reorder(data):\n",
    "    '''\n",
    "    reorder the data to this order:\n",
    "    ask1_v, ask1_p, bid1_v, bid1_p ... ask10_v, ask10_p, bid10_v, bid10_p\n",
    "    '''\n",
    "    data=np.array(data)\n",
    "    data=data.reshape(data.shape[0], 4, 10)\n",
    "    data= np.transpose(data, (0,2,1))\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    return data\n",
    "\n",
    "def data_classification(X, Y, T):\n",
    "    [N, D] = X.shape\n",
    "    df = np.array(X)\n",
    "\n",
    "    dY = np.array(Y)\n",
    "\n",
    "    dataY = dY[T - 1:N]\n",
    "\n",
    "    dataX = np.zeros((N - T + 1, T, D))\n",
    "    for i in range(T, N + 1):\n",
    "        dataX[i - T] = df[i - T:i, :]\n",
    "\n",
    "    return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "\n",
    "def price_legal_check(ask_price, bid_price):\n",
    "    # legal check\n",
    "    ask_price = math.ceil(100*ask_price)/100\n",
    "    bid_price = math.floor(100*bid_price)/100\n",
    "    return ask_price, bid_price\n",
    "\n",
    "def getRealizedVolatility(data, resample='min'):\n",
    "    if resample:\n",
    "        data = data.resample(resample).last()\n",
    "\n",
    "    midprice_lag = data.shift(1)\n",
    "    midprice_log = data.apply(np.log)\n",
    "    midprice_lag_log = midprice_lag.apply(np.log)\n",
    "    r = midprice_log - midprice_lag_log\n",
    "    r2 = r*r\n",
    "    rv = r2.sum()\n",
    "\n",
    "    return rv\n",
    "\n",
    "def getRelativeStrengthIndex(data):\n",
    "    length = len(data)\n",
    "    data = data.resample('s').last()\n",
    "    data = data.pct_change(1)\n",
    "    gain = data[data>0].sum()/length\n",
    "    loss = -data[data<0].sum()/length\n",
    "    if gain or loss:\n",
    "        rsi = gain/(gain+loss)\n",
    "    else:\n",
    "        rsi = .5\n",
    "    return rsi\n",
    "\n",
    "def getOrderStrengthIndex(data):\n",
    "    '''\n",
    "    data: msg\n",
    "    columns:[market_buy_volume  market_buy_n  market_sell_volume  market_sell_n  limit_buy_volume  limit_buy_n  limit_sell_volume  limit_sell_n  withdraw_buy_volume  withdraw_buy_n  withdraw_sell_volume  withdraw_sell_n]\n",
    "    '''\n",
    "    market_volume_intensity = (data.market_buy_volume.sum() - data.market_sell_volume.sum())/(data.market_buy_volume.sum() + data.market_sell_volume.sum() + 1e-7)\n",
    "    market_number_intensity = (data.market_buy_n.sum() - data.market_sell_n.sum())/(data.market_buy_n.sum() + data.market_sell_n.sum() + 1e-7)\n",
    "    limit_volume_intensity = (data.limit_buy_volume.sum() - data.limit_sell_volume.sum())/(data.limit_buy_volume.sum() + data.limit_sell_volume.sum() + 1e-7)\n",
    "    limit_number_intensity = (data.limit_buy_n.sum() - data.limit_sell_n.sum())/(data.limit_buy_n.sum() + data.limit_sell_n.sum() + 1e-7)\n",
    "    withdraw_volume_intensity = (data.withdraw_buy_volume.sum() - data.withdraw_sell_volume.sum())/(data.withdraw_buy_volume.sum() + data.withdraw_sell_volume.sum() + 1e-7)\n",
    "    withdraw_number_intensity = (data.withdraw_buy_n.sum() - data.withdraw_sell_n.sum())/(data.withdraw_buy_n.sum() + data.withdraw_sell_n.sum() + 1e-7)\n",
    "\n",
    "    return market_volume_intensity, market_number_intensity, limit_volume_intensity, limit_number_intensity, withdraw_volume_intensity, withdraw_number_intensity\n",
    "\n",
    "#base_env\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from tensorforce import Environment\n",
    "\n",
    "\n",
    "\n",
    "TRADE_UNIT = 100\n",
    "\n",
    "class BaseEnv():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            initial_value=0,\n",
    "            max_episode_timesteps=1000,\n",
    "            data_dir='./data',\n",
    "            log=1,\n",
    "            experiment_name='',\n",
    "            **kwargs\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.name = ''\n",
    "        self.initial_value = initial_value\n",
    "        self.__max_episode_timesteps__=max_episode_timesteps\n",
    "        self.data_dir = data_dir\n",
    "        self.log = log\n",
    "        self.exp_name = experiment_name\n",
    "\n",
    "    '''\n",
    "        You need to overload these functions\n",
    "    '''\n",
    "\n",
    "    def states(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def actions(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def action2order(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_state_at_t(self, t):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_reward(self, trade_price, trade_volume):\n",
    "        # Define reward function here\n",
    "        reward = self.value - self.value_\n",
    "        self.value_ = self.value\n",
    "        return reward\n",
    "\n",
    "    '''\n",
    "        Load data\n",
    "    '''\n",
    "\n",
    "    def load_orderbook(self, code, day):\n",
    "        #ask = pd.read_csv(self.data_dir + f'/{code}/{day}/ask.csv')\n",
    "        #bid = pd.read_csv(self.data_dir + f'/{code}/{day}/bid.csv').drop(['timestamp'], axis = 1)\n",
    "\n",
    "        #self.orderbook = pd.concat([ask, bid], axis=1)\n",
    "        self.orderbook = pd.read_csv(f\"data/{code}/{day}/{order_book_file}\")\n",
    "        self.orderbook = self.orderbook.drop('SecurityID', axis=1)\n",
    "        self.orderbook.timestamp = pd.to_datetime(self.orderbook.timestamp)\n",
    "        #self.orderbook = self.orderbook[(f'{self.day} 09:30:00'<self.orderbook.timestamp)&(self.orderbook.timestamp<f'{self.day} 14:57:00')]\n",
    "        self.orderbook = self.orderbook.set_index('timestamp')\n",
    "        self.orderbook_length = len(self.orderbook)\n",
    "        #print('load lob done!', code, day)\n",
    "        #print('loaded order book', len(self.orderbook))\n",
    "\n",
    "    def load_orderqueue(self, code, day):\n",
    "        pass\n",
    "\n",
    "    def load_price(self, code, day):\n",
    "        self.price = pd.read_csv(self.data_dir + f'/{code}/{day}/{price_file}')\n",
    "        #print(self.price.head())\n",
    "\n",
    "\n",
    "        self.price.timestamp = pd.to_datetime(self.price.timestamp)\n",
    "        self.price = self.price.set_index('timestamp')\n",
    "        #print(\"loaded price\", len(self.price))\n",
    "        #self.price = self.price.loc[self.orderbook.index]\n",
    "        #print(\"loaded price\", len(self.price))\n",
    "\n",
    "    def load_msg(self, code, day):\n",
    "        self.msg = pd.read_csv(self.data_dir + f'/{code}/{day}/{msg_file}')\n",
    "        #print('loaded msg', len(self.msg))\n",
    "        self.msg.timestamp = pd.to_datetime(self.msg.timestamp)\n",
    "        self.msg = self.msg.set_index('timestamp')\n",
    "        #self.msg = self.msg.loc[self.orderbook.index]\n",
    "\n",
    "    def load_order(self, code, day):\n",
    "        #order_columns = pd.read_csv('raw/GTA_SZL2_ORDER.csv')\n",
    "        #self.order = pd.read_csv(f'raw/SZL2_ORDER_{code}_{day[:6]}.csv', names=list(order_columns), low_memory=False)\n",
    "        self.order = pd.read_csv(f\"data/{code}/{day}/{order_file}\")\n",
    "\n",
    "        self.order.TradingTime = pd.to_datetime(self.order.TradingTime)\n",
    "        #self.order = self.order[self.order.TradingDate==int(day)]\n",
    "        self.order = self.order[(f'{self.day} 09:30:00'<self.order.TradingTime)&(self.order.TradingTime<f'{self.day} 14:57:00')]\n",
    "        #print(\"loaded order\", len(self.order))\n",
    "\n",
    "    def load_trade(self, code, day):\n",
    "        #trade_columns = pd.read_csv('raw/GTA_SZL2_TRADE.csv')\n",
    "        #self.trade = pd.read_csv(f'raw/SZL2_TRADE_{code}_{day[:6]}.csv', names=list(trade_columns))\n",
    "        self.trade = pd.read_csv(f\"data/{code}/{day}/{trade_file}\")\n",
    "\n",
    "        self.trade.TradingTime = pd.to_datetime(self.trade.TradingTime)\n",
    "\n",
    "        #self.trade = self.trade[self.trade.TradingDate==int(day)]\n",
    "        #self.trade = self.trade[self.trade.TradeType==\"F\"]\n",
    "        #print(\"loaded trades \", len(self.trade))\n",
    "        #print(\"all trading time \", set(self.trade.TradingTime))\n",
    "        self.trade = self.trade[self.trade['TradingTime'].dt.time.between(time(9, 30), time(14, 57))]\n",
    "        #print(\"loaded trades \", len(self.trade))\n",
    "        #self.trade = self.trade[(f'{self.day} 09:30:00'<self.trade.TradingTime)&(self.trade.TradingTime<f'{self.day} 14:57:00')]\n",
    "\n",
    "        #print(\"order book index \", self.orderbook.index)\n",
    "        self.is_trade = pd.DataFrame(index=self.orderbook.index,columns=['is_trade'])\n",
    "        self.is_trade['is_trade'] = 0\n",
    "        #print(\"all trading time \", set(self.trade.TradingTime))\n",
    "        self.is_trade.loc[set(self.trade.TradingTime)] = 1\n",
    "        #print(\"length of is_trade \", len(self.is_trade))\n",
    "        #print(self.is_trade.value_counts())\n",
    "\n",
    "    '''\n",
    "        Common function\n",
    "    '''\n",
    "\n",
    "    def reset_seq(self, timesteps_per_episode=None, episode_idx=None):\n",
    "        self.episode_idx = episode_idx\n",
    "        #print('timesteps_per_episode ', timesteps_per_episode)\n",
    "        if timesteps_per_episode == None:\n",
    "            self.episode_start = 0\n",
    "            self.episode_end = len(self.orderbook)\n",
    "            self.episode_state = self.orderbook\n",
    "        else:\n",
    "            self.episode_start = timesteps_per_episode * episode_idx\n",
    "            self.episode_end = min(self.episode_start + timesteps_per_episode, len(self.orderbook))\n",
    "            self.episode_state = self.orderbook.iloc[self.episode_start:self.episode_end]\n",
    "\n",
    "        self.episode_length = len(self.episode_state)\n",
    "\n",
    "        #print(\"self.episode_start, self.episode_end \", self.episode_start, self.episode_end)\n",
    "        episode_is_trade = self.is_trade.iloc[self.episode_start:self.episode_end]\n",
    "        #print(\"self.is_trade \", self.is_trade)\n",
    "        #print(\"episode_is_trade \", episode_is_trade)\n",
    "        has_trade_index = np.where(episode_is_trade==1)[0]\n",
    "        #ignore first T=50\n",
    "        has_trade_index = has_trade_index[has_trade_index>self.T]\n",
    "        #print(\"has_trade_index \", len(has_trade_index))\n",
    "        #print(\"has_trade_index \", has_trade_index)\n",
    "        self.index_iterator = iter(has_trade_index)\n",
    "\n",
    "\n",
    "        self.cash = self.value_ = self.value = self.initial_value\n",
    "        self.holding_pnl_total = self.trading_pnl_total = 0\n",
    "        self.inventory = 0\n",
    "        self.volume = 0\n",
    "        self.episode_reward = 0\n",
    "        self.mid_price_ = None\n",
    "        self.action_his = []\n",
    "        self.reward_dampened_pnl = 0\n",
    "        self.reward_trading_pnl = 0\n",
    "        self.reward_inventory_punishment = 0\n",
    "        self.reward_spread_punishment = 0\n",
    "\n",
    "        # log for trade\n",
    "        self.logger = self.price.iloc[self.episode_start:self.episode_end].copy()\n",
    "        #print(\"self.logger length, \", len(self.logger))\n",
    "        columns=['ask_price', 'bid_price', 'trade_price', 'trade_volume', 'value', 'volume', 'cash', 'inventory']\n",
    "        for column in columns:\n",
    "            self.logger[column] = np.nan\n",
    "\n",
    "        self.i = next(self.index_iterator)\n",
    "        self.i_ = next(self.index_iterator)\n",
    "        state = self.get_state_at_t(self.i-self.latency)\n",
    "\n",
    "        if self.log >= 1:\n",
    "            print(f'Reset env {self.name} {self.code}, {self.day}, from {self.episode_state.index[0]} to {self.episode_state.index[-1]}')\n",
    "            self.pbar = tqdm(total=self.episode_length)\n",
    "            self.pbar.update(self.i)\n",
    "\n",
    "        return state\n",
    "\n",
    "    def reset_random(self, timesteps_per_episode=2000):\n",
    "        self.episode_start = np.random.randint(0, len(self.orderbook) - timesteps_per_episode)\n",
    "        self.episode_end = min(self.episode_start + timesteps_per_episode, len(self.orderbook))\n",
    "        self.episode_state = self.orderbook.iloc[self.episode_start:self.episode_end]\n",
    "\n",
    "        self.episode_length = len(self.episode_state)\n",
    "\n",
    "        episode_is_trade = self.is_trade.iloc[self.episode_start:self.episode_end]\n",
    "        has_trade_index = np.where(episode_is_trade==1)[0]\n",
    "        #print(\"has_trade_index \", len(has_trade_index))\n",
    "        has_trade_index = has_trade_index[has_trade_index>self.T]\n",
    "        self.index_iterator = iter(has_trade_index)\n",
    "\n",
    "        self.cash = self.value_ = self.value = self.initial_value\n",
    "        self.holding_pnl_total = self.trading_pnl_total = 0\n",
    "        self.inventory = 0\n",
    "        self.volume = 0\n",
    "        self.episode_reward = 0\n",
    "        self.mid_price_ = None\n",
    "        self.action_his = []\n",
    "        self.reward_dampened_pnl = 0\n",
    "        self.reward_trading_pnl = 0\n",
    "        self.reward_inventory_punishment = 0\n",
    "        self.reward_spread_punishment = 0\n",
    "\n",
    "        # log for trade\n",
    "        self.logger = self.price.iloc[self.episode_start:self.episode_end].copy()\n",
    "        columns=['ask_price', 'bid_price', 'trade_price', 'trade_volume', 'value', 'volume', 'cash', 'inventory']\n",
    "        for column in columns:\n",
    "            self.logger[column] = np.nan\n",
    "\n",
    "        self.i = next(self.index_iterator)\n",
    "        self.i_ = next(self.index_iterator)\n",
    "        state = self.get_state_at_t(self.i-self.latency)\n",
    "\n",
    "        if self.log:\n",
    "            print(f'Reset env {self.name} {self.code}, {self.day}, from {self.episode_state.index[0]} to {self.episode_state.index[-1]}')\n",
    "            self.pbar = tqdm(total=self.episode_length)\n",
    "            self.pbar.update(self.i)\n",
    "\n",
    "        return state\n",
    "\n",
    "    def execute(self, actions):\n",
    "        self.action_his.append(actions)\n",
    "        # t\n",
    "        self.mid_price, self.ask1_price, self.bid1_price, self.lob_spread = self.get_price_info(self.i)\n",
    "        if self.mid_price_ == None:\n",
    "            self.mid_price_ = self.mid_price\n",
    "\n",
    "        orders = self.action2order(actions)\n",
    "        # inventory limit\n",
    "        if self.inventory < -10*TRADE_UNIT:\n",
    "            orders['ask_price']=0\n",
    "        elif self.inventory > 10*TRADE_UNIT:\n",
    "            orders['bid_price']=0\n",
    "\n",
    "        trade_price, trade_volume = self.match(orders)\n",
    "\n",
    "        self.update_agent(trade_price, trade_volume)\n",
    "\n",
    "        # log for trade result\n",
    "        if self.i >= len(self.logger):\n",
    "           print(\"index overflow, \", self.i, len(self.logger))\n",
    "        else:\n",
    "           self.logger.iloc[self.i, -8:] = [orders['ask_price'], orders['bid_price'], trade_price, trade_volume, self.value, self.volume, self.cash, self.inventory]\n",
    "\n",
    "        # if trade_volume:\n",
    "        #     print(self.i, 'ask1:', self.ask1_price, 'bid1:', self.bid1_price, 'buy' if trade_volume>0 else 'sell', 'at', trade_price)\n",
    "        if self.log >= 1:\n",
    "            self.pbar.update(self.i_ - self.i)\n",
    "\n",
    "        self.i = self.i_\n",
    "        # Termination conditions\n",
    "        terminal = False\n",
    "        try:\n",
    "            self.i_ = next(self.index_iterator)\n",
    "        except:\n",
    "            terminal = True\n",
    "\n",
    "        reward = self.get_reward(trade_price, trade_volume)\n",
    "        self.mid_price_ = self.mid_price\n",
    "\n",
    "        # close position\n",
    "        if terminal:\n",
    "            trade_price, trade_volume = self.close_position()\n",
    "            reward += self.get_reward(trade_price, trade_volume)\n",
    "\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        # log for result\n",
    "        if terminal:\n",
    "            self.post_experiment(False)\n",
    "\n",
    "        state = self.get_state_at_t(self.i-self.latency)\n",
    "\n",
    "        return state, terminal, reward\n",
    "\n",
    "    def match(self, actions):\n",
    "        trade_volume = 0\n",
    "        trade_price = 0\n",
    "        ask_price, ask_volume, bid_price, bid_volume = actions.values()\n",
    "\n",
    "        # trade\n",
    "        now_t = self.trade[self.trade.TradingTime==self.episode_state.index[self.i]]\n",
    "        now_trading_price_max = now_t.TradePrice.max()\n",
    "        now_trading_price_max_v = now_t[now_t.TradePrice==now_trading_price_max].TradeVolume.sum()\n",
    "        now_trading_price_min = now_t.TradePrice.min()\n",
    "        now_trading_price_min_v = now_t[now_t.TradePrice==now_trading_price_min].TradeVolume.sum()\n",
    "        #print(\"now_trading_price_min, now_trading_price_max\", now_trading_price_min, now_trading_price_max, now_trading_price_min_v, now_trading_price_max_v)\n",
    "\n",
    "        # t - 1\n",
    "        t_1_mid_price, t_1_a1_price, t_1_b1_price, t_1_spread = self.get_price_info(self.i-1)\n",
    "        #print(\"mid, bid, ask, spread \" , t_1_mid_price, t_1_b1_price, t_1_a1_price, t_1_spread)\n",
    "\n",
    "        # sell order\n",
    "        if ask_price and ask_volume:\n",
    "            if ask_price <= t_1_b1_price:\n",
    "                # market order\n",
    "                trade_price, trade_volume = t_1_b1_price, ask_volume\n",
    "                pnl = trade_volume * (t_1_mid_price - trade_price)\n",
    "                self.trading_pnl_total += pnl\n",
    "                #print(\"market order sell at \", trade_price, \", trading pnl \", pnl)\n",
    "            else:\n",
    "                # limit order\n",
    "                if now_trading_price_max > ask_price:\n",
    "                    # all deal\n",
    "                    trade_price, trade_volume = ask_price, ask_volume\n",
    "\n",
    "                    pnl = trade_volume * (t_1_mid_price - trade_price)\n",
    "                    self.trading_pnl_total += pnl\n",
    "                    #print(\"limit order sell at\", trade_price, \", trading pnl \", pnl, trade_volume)\n",
    "\n",
    "                # we assume that our quotes rest at the back of the queue\n",
    "                elif now_trading_price_max == ask_price:\n",
    "                    # deal probability: traded volume/all volume in this level\n",
    "                    lob_depth = self.episode_state.iloc[self.i].ask_quantity_1\n",
    "                    transac_prob = now_trading_price_max_v/(now_trading_price_max_v+lob_depth)\n",
    "                    is_transac = np.random.choice([1, 0], p=[transac_prob, 1-transac_prob])\n",
    "                    if is_transac:\n",
    "                        trade_price, trade_volume = ask_price, ask_volume\n",
    "\n",
    "        # buy order\n",
    "        if bid_price and bid_volume:\n",
    "            if bid_price >= t_1_a1_price:\n",
    "                # market order\n",
    "                trade_price, trade_volume = t_1_a1_price, bid_volume\n",
    "                pnl = trade_volume * (t_1_mid_price - trade_price)\n",
    "                self.trading_pnl_total += pnl\n",
    "                #print(\"market order buy at\", trade_price, \", trading pnl \", pnl)\n",
    "            else:\n",
    "                if now_trading_price_min < bid_price:\n",
    "                    trade_price, trade_volume = bid_price, bid_volume\n",
    "                    pnl = trade_volume * (t_1_mid_price - trade_price)\n",
    "                    self.trading_pnl_total += pnl\n",
    "                    #print(\"limit order buy at\", trade_price, \", trading pnl \", pnl)\n",
    "\n",
    "                # we assume that our quotes rest at the back of the queue\n",
    "                elif now_trading_price_min == bid_price:\n",
    "                    lob_depth = self.episode_state.iloc[self.i].bid_quantity_1\n",
    "                    transac_prob = now_trading_price_min_v/(now_trading_price_min_v+lob_depth)\n",
    "                    is_transac = np.random.choice([1, 0], p=[transac_prob, 1-transac_prob])\n",
    "                    if is_transac:\n",
    "                        trade_price, trade_volume = bid_price, bid_volume\n",
    "\n",
    "        return trade_price, trade_volume\n",
    "\n",
    "    def close_position(self):\n",
    "        # t - 1\n",
    "        t_1_mid_price, t_1_a1_price, t_1_b1_price, t_1_spread = self.get_price_info(self.i-1)\n",
    "\n",
    "        # Market order\n",
    "        if self.inventory < 0:\n",
    "            # Buy\n",
    "            trade_price, trade_volume = t_1_b1_price, -self.inventory\n",
    "            self.volume += trade_volume\n",
    "        elif self.inventory > 0:\n",
    "            # Sell\n",
    "            trade_price, trade_volume = t_1_a1_price, -self.inventory\n",
    "        else:\n",
    "            trade_price, trade_volume = 0, 0\n",
    "\n",
    "        self.update_agent(trade_price, trade_volume)\n",
    "\n",
    "        # log for trade result\n",
    "        self.logger.iloc[self.i, -6:] = [trade_price, trade_volume, self.value, self.volume, self.cash, self.inventory]\n",
    "\n",
    "        return trade_price, trade_volume\n",
    "\n",
    "    def update_agent(self, trade_price, trade_volume):\n",
    "        self.inventory_ = self.inventory\n",
    "        self.inventory += trade_volume\n",
    "        self.cash -= trade_volume*trade_price\n",
    "        self.value = self.get_value(self.mid_price)\n",
    "\n",
    "        volume = max(0, trade_volume*trade_price) # only count for buy\n",
    "        self.volume += volume\n",
    "\n",
    "    def get_price_info(self, i):\n",
    "        price = self.price[self.price.index==self.episode_state.index[i]].iloc[-1]\n",
    "        #print(price)\n",
    "\n",
    "        bid1_price = price.bid1_price.item()\n",
    "        ask1_price = price.ask1_price.item()\n",
    "        bid1_price, ask1_price = round(bid1_price,2), round(ask1_price,2)\n",
    "        mid_price = (bid1_price+ask1_price)/2\n",
    "        spread = ask1_price - bid1_price\n",
    "\n",
    "        return mid_price, ask1_price, bid1_price, spread\n",
    "\n",
    "    def get_value(self, price):\n",
    "        return self.cash + self.inventory*price\n",
    "\n",
    "    '''\n",
    "        For evaluation and save trading log\n",
    "    '''\n",
    "\n",
    "    def post_experiment(self, save=False):\n",
    "        logger_wo_exit_market = self.logger[(self.logger.ask_price != 0) & (self.logger.bid_price != 0)]\n",
    "        self.episode_avg_spread = (logger_wo_exit_market.ask_price - logger_wo_exit_market.bid_price).mean()\n",
    "        self.episode_avg_position = self.logger.inventory.mean()\n",
    "        self.episode_avg_abs_position = self.logger.inventory.abs().mean()\n",
    "        self.episode_profit_ratio = self.value/(self.volume+1e-7)\n",
    "        self.pnl = self.value - self.initial_value\n",
    "        self.holding_pnl_total = self.pnl - self.trading_pnl_total\n",
    "        self.nd_pnl = self.pnl/self.episode_avg_spread\n",
    "        self.pnl_map = self.pnl/(self.episode_avg_abs_position+1e-7)\n",
    "\n",
    "        if self.log >= 1:\n",
    "            print(\n",
    "                \"PnL:\", self.pnl,\n",
    "                \"Holding PnL\", self.holding_pnl_total,\n",
    "                \"Trading PnL\", self.trading_pnl_total,\n",
    "                \"ND-PnL(pnl/spread):\", self.nd_pnl,\n",
    "                \"PnL-MAP(pnl/pos):\", self.pnl_map,\n",
    "                \"Trading volume:\", self.volume,\n",
    "                \"Profit ratio:\", self.episode_profit_ratio,\n",
    "                \"Averaged position:\",self.episode_avg_position,\n",
    "                \"Averaged Abs position:\",self.episode_avg_abs_position,\n",
    "                \"Averaged spread:\", self.episode_avg_spread,\n",
    "                \"Episodic reward:\", self.episode_reward\n",
    "                )\n",
    "            self.pbar.close()\n",
    "\n",
    "        if self.log >= 2:\n",
    "            trade_log = self.logger[(self.logger.trade_volume > 0)|(self.logger.trade_volume < 0)]\n",
    "            for i in range(len(trade_log)):\n",
    "                item = trade_log.iloc[i]\n",
    "                if item.trade_volume > 0:\n",
    "                    print(item.name, 'BUY at', item.trade_price, 'inventory', item.inventory, 'value', item.value)\n",
    "                elif item.trade_volume < 0:\n",
    "                    print(item.name, 'SELL at', item.trade_price, 'inventory', item.inventory, 'value', item.value)\n",
    "\n",
    "        if save:\n",
    "            now_time = time_module.strftime('%Y_%m_%d_%H_%M_%S', time_module.localtime())\n",
    "            log_file = f\"./log/{self.exp_name}_{self.code}_{self.day}_{now_time}.csv\"\n",
    "            self.logger.to_csv(log_file)\n",
    "            print(\"Trading log saved to\", log_file)\n",
    "\n",
    "    def get_final_result(self):\n",
    "        return dict(\n",
    "            pnl=self.pnl,\n",
    "            nd_pnl=self.nd_pnl,\n",
    "            pnl_map=self.pnl_map,\n",
    "            profit_ratio=self.episode_profit_ratio,\n",
    "            avg_position=self.episode_avg_position,\n",
    "            avg_abs_position=self.episode_avg_abs_position,\n",
    "            avg_spread=self.episode_avg_spread,\n",
    "            volume=self.volume,\n",
    "            episode_reward=self.episode_reward\n",
    "        )\n",
    "\n",
    "#env_feature\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "class EnvFeature(BaseEnv):\n",
    "    \"\"\"\n",
    "        Use this class to calculate your factor\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _get_market_state(self,t):\n",
    "        data_300s = self.price[(self.price.index<=self.episode_state.index[t])&(self.price.index>=self.episode_state.index[t]-timedelta(seconds=300))].midprice\n",
    "        data_600s = self.price[(self.price.index<=self.episode_state.index[t])&(self.price.index>=self.episode_state.index[t]-timedelta(seconds=600))].midprice\n",
    "        data_1800s = self.price[(self.price.index<=self.episode_state.index[t])&(self.price.index>=self.episode_state.index[t]-timedelta(seconds=1800))].midprice\n",
    "        rv_300s = getRealizedVolatility(data_300s,resample='s')*1e4\n",
    "        rv_600s = getRealizedVolatility(data_600s,resample='s')*1e4\n",
    "        rv_1800s = getRealizedVolatility(data_1800s,resample='s')*1e4\n",
    "        rsi_300s = getRelativeStrengthIndex(data_300s)\n",
    "        rsi_600s = getRelativeStrengthIndex(data_600s)\n",
    "        rsi_1800s = getRelativeStrengthIndex(data_1800s)\n",
    "        return [rv_300s, rv_600s, rv_1800s, rsi_300s, rsi_600s, rsi_1800s]\n",
    "\n",
    "    def _get_order_strength_index(self,t):\n",
    "        data_10s = self.msg[(self.msg.index<=self.episode_state.index[t])&(self.msg.index>=self.episode_state.index[t]-timedelta(seconds=10))]\n",
    "        data_60s = self.msg[(self.msg.index<=self.episode_state.index[t])&(self.msg.index>=self.episode_state.index[t]-timedelta(seconds=60))]\n",
    "        data_300s = self.msg[(self.msg.index<=self.episode_state.index[t])&(self.msg.index>=self.episode_state.index[t]-timedelta(seconds=300))]\n",
    "\n",
    "        svi_10s, sni_10s, lvi_10s, lni_10s, wvi_10s, wni_10s = getOrderStrengthIndex(data_10s)\n",
    "        svi_60s, sni_60s, lvi_60s, lni_60s, wvi_60s, wni_60s = getOrderStrengthIndex(data_60s)\n",
    "        svi_300s, sni_300s, lvi_300s, lni_300s, wvi_300s, wni_300s = getOrderStrengthIndex(data_300s)\n",
    "        return [0, 0, 0, 0, 0, 0,\n",
    "                0, 0, 0, 0, 0, 0,\n",
    "                0, 0, 0, 0, 0, 0,\n",
    "                ]\n",
    "\n",
    "#        return [\n",
    "#            svi_10s, sni_10s, lvi_10s, lni_10s, wvi_10s, wni_10s,\n",
    "#            svi_60s, sni_60s, lvi_60s, lni_60s, wvi_60s, wni_60s,\n",
    "#            svi_300s, sni_300s, lvi_300s, lni_300s, wvi_300s, wni_300s\n",
    "#        ]\n",
    "#\n",
    "\n",
    "\n",
    "#env_continuous\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "class EnvContinuous(EnvFeature):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            code='600519',\n",
    "            day='20191101',\n",
    "            latency=0,\n",
    "            T=50,\n",
    "            # ablation states\n",
    "            wo_lob_state=False,\n",
    "            wo_market_state=False,\n",
    "            wo_agent_state=False,\n",
    "            # ablation rewards\n",
    "            wo_dampened_pnl=False,\n",
    "            wo_matched_pnl=False,\n",
    "            wo_inv_punish=False,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.name = \"Continuous\"\n",
    "        print(\"Environment:\", self.name)\n",
    "        self.code = code\n",
    "        self.day = day2date(day)\n",
    "\n",
    "        self.latency = latency\n",
    "        self.T = T\n",
    "\n",
    "        # ablation\n",
    "        self.wo_lob_state = wo_lob_state\n",
    "        self.wo_market_state = wo_market_state\n",
    "        self.wo_agent_state = wo_agent_state\n",
    "        self.r_da = 0 if wo_dampened_pnl else 1\n",
    "        self.r_ma = 0 if wo_matched_pnl else 1\n",
    "        self.r_ip = 0 if wo_inv_punish else 1\n",
    "\n",
    "        # Inventory punishment factor\n",
    "        self.theta = 0.01\n",
    "        self.eta = 0.9\n",
    "\n",
    "        self.init_states()\n",
    "\n",
    "        self.load_orderbook(code=code, day=day)\n",
    "        self.load_price(code=code, day=day)\n",
    "        self.load_trade(code=code, day=day)\n",
    "        self.load_msg(code=code, day=day)\n",
    "\n",
    "    def init_states(self):\n",
    "        self.__states_space__ = dict()\n",
    "        if not self.wo_lob_state:\n",
    "            self.__states_space__['lob_state'] = dict(\n",
    "                type='float',\n",
    "                shape=(self.T,40,1)\n",
    "                )\n",
    "        if not self.wo_market_state:\n",
    "            self.__states_space__['market_state'] = dict(\n",
    "                type='float',\n",
    "                shape=(24,)\n",
    "                )\n",
    "        if not self.wo_agent_state:\n",
    "            self.__states_space__['agent_state'] = dict(\n",
    "                type='float',\n",
    "                shape=(24,)\n",
    "                )\n",
    "\n",
    "    def states(self):\n",
    "        return self.__states_space__\n",
    "\n",
    "    def actions(self):\n",
    "        return dict(\n",
    "                    type='float',\n",
    "                    shape=(2,),\n",
    "                    min_value=-1,\n",
    "                    max_value=1\n",
    "                )\n",
    "\n",
    "    def max_episode_timesteps(self):\n",
    "        return self.__max_episode_timesteps__\n",
    "\n",
    "    def action2order(self, actions):\n",
    "        # t-latency\n",
    "        t_1_mid_price, t_1_a1_price, t_1_b1_price, t_1_spread = self.get_price_info(self.i-self.latency)\n",
    "\n",
    "        # action 1\n",
    "        # actions in [0, 1]\n",
    "        delta_price = actions[0]*0.05\n",
    "        spread = actions[1]*0.1\n",
    "        if self.inventory > 0:\n",
    "            reservation = t_1_mid_price - delta_price\n",
    "        elif self.inventory < 0:\n",
    "            reservation = t_1_mid_price + delta_price\n",
    "        else:\n",
    "            reservation = t_1_mid_price\n",
    "        ask_price = reservation + spread/2\n",
    "        bid_price = reservation - spread/2\n",
    "\n",
    "        # action 2\n",
    "        # actions in [-1, 1]\n",
    "        # delta_price = actions[0]*0.05\n",
    "        # spread = abs(actions[1])*0.1\n",
    "        # reservation = t_1_mid_price - delta_price\n",
    "        # ask_price = reservation + spread/2\n",
    "        # bid_price = reservation - spread/2\n",
    "\n",
    "        # action 3\n",
    "        # actions in [0, 1]\n",
    "        # ask_price = t_1_a1_price + actions[0]*0.1\n",
    "        # bid_price = t_1_b1_price - actions[1]*0.1\n",
    "        # reservation = (ask_price + bid_price)/2\n",
    "        # spread = ask_price - bid_price\n",
    "\n",
    "        ask_price, bid_price = price_legal_check(ask_price, bid_price)\n",
    "\n",
    "        # save for log\n",
    "        self.reservation = reservation\n",
    "        self.spread = spread\n",
    "\n",
    "        orders = {\n",
    "            'ask_price': ask_price,\n",
    "            'ask_vol': -TRADE_UNIT,\n",
    "            'bid_price': bid_price,\n",
    "            'bid_vol': TRADE_UNIT\n",
    "        }\n",
    "        return orders\n",
    "\n",
    "    def get_reward(self, trade_price, trade_volume):\n",
    "        pnl = self.value - self.value_\n",
    "\n",
    "        # Asymmetrically dampened PnL\n",
    "        asymmetric_dampen = max(0, self.eta * pnl)\n",
    "        dampened_pnl = pnl - asymmetric_dampen\n",
    "\n",
    "        matched_pnl = (self.mid_price - trade_price) * trade_volume\n",
    "\n",
    "        # delta_inventory = abs(self.inventory) - abs(self.inventory_)\n",
    "        # delta_inventory = max(0, delta_inventory)\n",
    "        # inventory_punishment = self.theta * (delta_inventory/TRADE_UNIT)\n",
    "\n",
    "        inventory_punishment = self.theta * (self.inventory/TRADE_UNIT)**2\n",
    "\n",
    "        # spread punishment\n",
    "        if self.inventory:\n",
    "            spread_punishment = 0\n",
    "        else:\n",
    "            spread_punishment = 100*self.spread if self.spread > 0.02 else 0\n",
    "\n",
    "        reward = pnl - spread_punishment#self.r_ma * matched_pnl + self.r_da * dampened_pnl - self.r_ip * inventory_punishment - spread_punishment\n",
    "\n",
    "        self.value_ = self.value\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def get_state_at_t(self, t):\n",
    "        self.__state__ = dict()\n",
    "\n",
    "        if not self.wo_lob_state:\n",
    "            lob = self.episode_state.iloc[t-self.T:t]\n",
    "            #print(\"lob \", lob)\n",
    "            mid_price = (lob.ask_price_1 + lob.bid_price_1)/2\n",
    "            lob_normed = lob_norm(lob, mid_price)\n",
    "            self.__state__['lob_state'] = np.expand_dims(np.array(lob_normed), -1)\n",
    "\n",
    "        if not self.wo_market_state:\n",
    "            self.__state__['market_state'] = self._get_market_state(t) + self._get_order_strength_index(t)\n",
    "\n",
    "        if not self.wo_agent_state:\n",
    "            self.__state__['agent_state'] = [self.inventory/(10*TRADE_UNIT)]*12 + [t / self.episode_length]*12\n",
    "\n",
    "        return self.__state__\n",
    "\n",
    "\n",
    "\n",
    "#env_discret\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EnvDiscrete(EnvFeature):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            code='000001',\n",
    "            day='20191101',\n",
    "            data_norm=True,\n",
    "            latency=0,\n",
    "            T=50,\n",
    "            # ablation states\n",
    "            wo_lob_state=False,\n",
    "            wo_market_state=False,\n",
    "            wo_agent_state=False,\n",
    "            # ablation rewards\n",
    "            wo_dampened_pnl=False,\n",
    "            wo_matched_pnl=False,\n",
    "            wo_inv_punish=False,\n",
    "            **kwargs\n",
    "        ):\n",
    "        super().__init__(**kwargs)\n",
    "        print(\"Environment: EnvDiscrete\")\n",
    "        self.code = code\n",
    "        self.day = day2date(day)\n",
    "\n",
    "        self.latency = latency\n",
    "        self.T = T\n",
    "\n",
    "        # ablation\n",
    "        self.wo_lob_state = wo_lob_state\n",
    "        self.wo_market_state = wo_market_state\n",
    "        self.wo_agent_state = wo_agent_state\n",
    "        self.r_da = 0 if wo_dampened_pnl else 1\n",
    "        self.r_ma = 0 if wo_matched_pnl else 1\n",
    "        self.r_ip = 0 if wo_inv_punish else 1\n",
    "\n",
    "        # Inventory punishment factor\n",
    "        self.theta = 0.01\n",
    "        self.eta = 0.5\n",
    "\n",
    "        self.init_states()\n",
    "\n",
    "        self.load_orderbook(code=code, day=day)\n",
    "        self.load_price(code=code, day=day)\n",
    "        self.load_trade(code=code, day=day)\n",
    "        self.load_msg(code=code, day=day)\n",
    "\n",
    "    def init_states(self):\n",
    "        self.__states_space__ = dict()\n",
    "        if not self.wo_lob_state:\n",
    "            self.__states_space__['lob_state'] = dict(\n",
    "                type='float',\n",
    "                shape=(self.T,40,1)\n",
    "                )\n",
    "        if not self.wo_market_state:\n",
    "            self.__states_space__['market_state'] = dict(\n",
    "                type='float',\n",
    "                shape=(24,)\n",
    "                )\n",
    "        if not self.wo_agent_state:\n",
    "            self.__states_space__['agent_state'] = dict(\n",
    "                type='float',\n",
    "                shape=(24,)\n",
    "                )\n",
    "\n",
    "    def states(self):\n",
    "        return self.__states_space__\n",
    "\n",
    "    def actions(self):\n",
    "        return dict(\n",
    "                    type='int',\n",
    "                    num_values=5\n",
    "                )\n",
    "\n",
    "    def max_episode_timesteps(self):\n",
    "        return self.__max_episode_timesteps__\n",
    "\n",
    "    def action2order(self, actions):\n",
    "        # t-latency\n",
    "        t_1_mid_price, t_1_a1_price, t_1_b1_price, t_1_spread = self.get_price_info(self.i-self.latency)\n",
    "\n",
    "        ask_price, bid_price = 0, 0\n",
    "        ask_volume, bid_volume = -TRADE_UNIT,TRADE_UNIT\n",
    "\n",
    "        if actions in range(7):\n",
    "            # limit order\n",
    "            if actions == 0:\n",
    "                ask_price = t_1_a1_price\n",
    "                bid_price = t_1_b1_price\n",
    "            elif actions == 1:\n",
    "                ask_price = t_1_a1_price\n",
    "                bid_price = t_1_b1_price-0.01\n",
    "            elif actions == 2:\n",
    "                ask_price = t_1_a1_price+0.01\n",
    "                bid_price = t_1_b1_price\n",
    "            elif actions == 3:\n",
    "                ask_price = t_1_a1_price+0.01\n",
    "                bid_price = t_1_b1_price-0.01\n",
    "            elif actions == 4:\n",
    "                ask_price = t_1_a1_price\n",
    "                bid_price = t_1_b1_price-0.02\n",
    "            elif actions == 5:\n",
    "                ask_price = t_1_a1_price+0.02\n",
    "                bid_price = t_1_b1_price\n",
    "            elif actions == 6:\n",
    "                ask_price = t_1_a1_price+0.02\n",
    "                bid_price = t_1_b1_price-0.02\n",
    "\n",
    "        elif actions==7:\n",
    "            # market order to clode position\n",
    "            if self.inventory < 0:\n",
    "                bid_price, bid_volume = np.inf, -self.inventory\n",
    "            elif self.inventory > 0:\n",
    "                ask_price, ask_volume = 0.01, -self.inventory\n",
    "            else:\n",
    "                trade_price, trade_volume = 0, 0\n",
    "\n",
    "        # inventory limit\n",
    "        if self.inventory < -10*TRADE_UNIT:\n",
    "            ask_price=0\n",
    "            ask_volume=0\n",
    "        elif self.inventory > 10*TRADE_UNIT:\n",
    "            bid_price=0\n",
    "            bid_volume=0\n",
    "\n",
    "        orders = {\n",
    "            'ask_price': ask_price,\n",
    "            'ask_vol': ask_volume,\n",
    "            'bid_price': bid_price,\n",
    "            'bid_vol': bid_volume\n",
    "        }\n",
    "\n",
    "        return orders\n",
    "\n",
    "    def get_reward(self, trade_price, trade_volume):\n",
    "        pnl = self.value - self.value_\n",
    "\n",
    "        # Asymmetrically dampened PnL\n",
    "        asymmetric_dampen = max(0, self.eta * pnl)\n",
    "        dampened_pnl = pnl - asymmetric_dampen\n",
    "\n",
    "        matched_pnl = (self.mid_price - trade_price) * trade_volume\n",
    "\n",
    "        delta_inventory = abs(self.inventory) - abs(self.inventory_)\n",
    "        # delta_inventory = max(0, delta_inventory)\n",
    "\n",
    "        inventory_punishment = self.theta * (delta_inventory/TRADE_UNIT)\n",
    "        # inventory_punishment = self.theta * (self.inventory/TRADE_UNIT)**2\n",
    "        reward = pnl\n",
    "        # reward = self.r_ma * matched_pnl + self.r_da * dampened_pnl - self.r_ip * inventory_punishment\n",
    "        self.value_ = self.value\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def get_state_at_t(self, t):\n",
    "        self.__state__ = dict()\n",
    "\n",
    "        if not self.wo_lob_state:\n",
    "            lob = self.episode_state.iloc[t-self.T:t]\n",
    "            mid_price = (lob.ask_price_1 + lob.bid_price_1)/2\n",
    "            lob_normed = lob_norm(lob, mid_price)\n",
    "            self.__state__['lob_state'] = np.expand_dims(np.array(lob_normed), -1)\n",
    "\n",
    "        if not self.wo_market_state:\n",
    "            self.__state__['market_state'] = self._get_market_state(t) + self._get_order_strength_index(t)\n",
    "\n",
    "        if not self.wo_agent_state:\n",
    "            self.__state__['agent_state'] = [self.inventory/(10*TRADE_UNIT)]*12 + [t / self.episode_length]*12\n",
    "\n",
    "        return self.__state__\n",
    "\n",
    "\n",
    "#agent\n",
    "from tensorforce.agents import Agent\n",
    "\n",
    "def get_dueling_dqn_agent(\n",
    "                        network,\n",
    "                        environment=None,\n",
    "                        states=None,\n",
    "                        actions=None,\n",
    "                        max_episode_timesteps=None,\n",
    "                        batch_size=32,\n",
    "                        learning_rate=1e-4,\n",
    "                        horizon=1,\n",
    "                        discount=0.99,\n",
    "                        memory=200000,\n",
    "                        device='gpu'\n",
    "                        ):\n",
    "    if environment != None:\n",
    "        agent = Agent.create(\n",
    "        agent='dueling_dqn',\n",
    "        environment=environment,\n",
    "        max_episode_timesteps=max_episode_timesteps,\n",
    "        network=network,\n",
    "        config=dict(device=device),\n",
    "        memory=memory,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        horizon=horizon,\n",
    "        discount=discount,\n",
    "        parallel_interactions=10,\n",
    "    )\n",
    "    else:\n",
    "        agent = Agent.create(\n",
    "            agent='dueling_dqn',\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            max_episode_timesteps=max_episode_timesteps,\n",
    "            network=network,\n",
    "            config=dict(device=device),\n",
    "            memory=memory,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            horizon=horizon,\n",
    "            discount=discount,\n",
    "            parallel_interactions=10,\n",
    "        )\n",
    "    return agent\n",
    "\n",
    "def get_ppo_agent(\n",
    "                network,\n",
    "                environment=None,\n",
    "                states=None,\n",
    "                actions=None,\n",
    "                max_episode_timesteps=None,\n",
    "                batch_size=32,\n",
    "                learning_rate=1e-3,\n",
    "                horizon=None,\n",
    "                discount=0.99,\n",
    "                device='gpu'\n",
    "                ):\n",
    "    if environment != None:\n",
    "        agent = Agent.create(\n",
    "            agent='ppo',\n",
    "            environment=environment,\n",
    "            max_episode_timesteps=max_episode_timesteps,\n",
    "            network=network,\n",
    "            config=dict(device=device),\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            discount=discount,\n",
    "            parallel_interactions=10,\n",
    "        )\n",
    "    else:\n",
    "        agent = Agent.create(\n",
    "            agent='ppo',\n",
    "            environment=environment,\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            max_episode_timesteps=max_episode_timesteps,\n",
    "            network=network,\n",
    "            config=dict(device=device),\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            discount=discount,\n",
    "            parallel_interactions=10,\n",
    "        )\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "\n",
    "#network\n",
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"-1\"\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth=True                                #按需分配显存\n",
    "K.set_session(tf.compat.v1.Session(config=config))\n",
    "\n",
    "def get_lob_model(latent_dim, T):\n",
    "    lob_state = keras.layers.Input(shape=(T, 40, 1))\n",
    "\n",
    "    conv_first1 = keras.layers.Conv2D(32, (1, 2), strides=(1, 2))(lob_state)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = keras.layers.Conv2D(32, (1, 5), strides=(1, 5))(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = keras.layers.Conv2D(32, (1, 4))(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = keras.layers.Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    # build the inception module\n",
    "    convsecond_1 = keras.layers.Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    convsecond_1 = keras.layers.Conv2D(64, (3, 1), padding='same')(convsecond_1)\n",
    "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "\n",
    "    convsecond_2 = keras.layers.Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    convsecond_2 = keras.layers.Conv2D(64, (5, 1), padding='same')(convsecond_2)\n",
    "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "\n",
    "    convsecond_3 = keras.layers.MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "    convsecond_3 = keras.layers.Conv2D(64, (1, 1), padding='same')(convsecond_3)\n",
    "    convsecond_3 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "\n",
    "    convsecond_output = keras.layers.concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "    conv_reshape = keras.layers.Reshape((int(convsecond_output.shape[1]), int(convsecond_output.shape[3])))(convsecond_output)\n",
    "\n",
    "    attn_input = conv_reshape\n",
    "    attn_input_last = attn_input[:,-1:,:]\n",
    "\n",
    "    multi_head_attn_layer_1 = keras.layers.MultiHeadAttention(num_heads=10, key_dim=16, output_shape=64)\n",
    "\n",
    "    attn_output, weight = multi_head_attn_layer_1(attn_input_last, attn_input, return_attention_scores=True)\n",
    "\n",
    "    attn_output = keras.layers.Flatten()(attn_output)\n",
    "\n",
    "    # add Batch Normalization\n",
    "    # attn_output = keras.layers.BatchNormalization()(attn_output)\n",
    "\n",
    "    # add Layer Normalization\n",
    "    # attn_output = keras.layers.LayerNormalization()(attn_output)\n",
    "\n",
    "    return keras.models.Model(lob_state, attn_output)\n",
    "\n",
    "\n",
    "def get_fclob_model(latent_dim,T):\n",
    "    print(\"This is the FC-LOB model\")\n",
    "    lob_state = keras.layers.Input(shape=(T, 40, 1))\n",
    "\n",
    "    dense_input = keras.layers.Flatten()(lob_state)\n",
    "\n",
    "    dense_output = keras.layers.Dense(1024, activation='leaky_relu')(dense_input)\n",
    "    dense_output = keras.layers.Dense(256, activation='leaky_relu')(dense_input)\n",
    "    dense_output = keras.layers.Dense(latent_dim, activation='leaky_relu')(dense_input)\n",
    "\n",
    "    return keras.models.Model(lob_state, dense_output)\n",
    "\n",
    "def compute_output_shape(input_shape):\n",
    "    return (input_shape[0], 64)\n",
    "\n",
    "def get_pretrain_model(model, T):\n",
    "    lob_state = keras.layers.Input(shape=(T, 40, 1))\n",
    "    embedding = model(lob_state)\n",
    "    output = keras.layers.Dense(3, activation='softmax')(embedding)\n",
    "\n",
    "    return keras.models.Model(lob_state, output)\n",
    "\n",
    "def get_model(lob_model, T, with_lob_state=True, with_market_state=True, with_agent_state=True):\n",
    "    input_ls = list()\n",
    "    dense_input = list()\n",
    "    if with_lob_state:\n",
    "        lob_state = keras.layers.Input(shape=(T, 40, 1))\n",
    "        encoder_outputs = lob_model(lob_state)\n",
    "        input_ls.append(lob_state)\n",
    "        dense_input.append(encoder_outputs)\n",
    "    else:\n",
    "        print('w/o lob state!')\n",
    "\n",
    "    if with_agent_state:\n",
    "        agent_state = keras.layers.Input(shape=(24,))\n",
    "        input_ls.append(agent_state)\n",
    "        dense_input.append(agent_state)\n",
    "    else:\n",
    "         print('w/o agent state!')\n",
    "\n",
    "    if with_market_state:\n",
    "        market_state = keras.layers.Input(shape=(24,))\n",
    "        input_ls.append(market_state)\n",
    "        dense_input.append(agent_state)\n",
    "    else:\n",
    "        print('w/o market state!')\n",
    "\n",
    "    dense_input = keras.layers.concatenate(dense_input, axis=1)\n",
    "\n",
    "    dense_output = keras.layers.Dense(64, activation='leaky_relu')(dense_input)\n",
    "\n",
    "    return keras.models.Model(input_ls, dense_output)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    get_lob_model(64,50).summary()\n",
    "\n",
    "#main\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "import pyrallis\n",
    "from dataclasses import asdict, dataclass\n",
    "\n",
    "from tensorforce.environments import Environment\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # Experiment\n",
    "    code: str = '600519'\n",
    "    device: str = \"cpu\"\n",
    "    latency: int = 0\n",
    "    time_window: int = 50\n",
    "    log: int = 1\n",
    "    exp_name: str = ''\n",
    "    # Agent\n",
    "    agent_type: str = 'ppo' # ppo/dueling dqn\n",
    "    #agent_type: str = 'dueling_dqn'\n",
    "    learning_rate: int = 1e-4\n",
    "    horizon: int = 1\n",
    "    env_type: str = 'continuous' # continuous/discrete\n",
    "    load: bool = False\n",
    "    agent_load_dir: str = 'agent'\n",
    "    save: bool = True,\n",
    "    agent_save_dir: str = 'agent'\n",
    "    # Ablation\n",
    "    wo_pretrain: bool = False\n",
    "    wo_attnlob: bool = False\n",
    "    wo_lob_state: bool = False\n",
    "    wo_market_state: bool = False\n",
    "    wo_dampened_pnl: bool = False\n",
    "    wo_matched_pnl: bool = False\n",
    "    wo_inv_punish: bool = False\n",
    "\n",
    "\n",
    "def init_env(day, config):\n",
    "    if config['env_type'] == 'continuous':\n",
    "        env = EnvContinuous\n",
    "    elif config['env_type'] == 'discrete':\n",
    "        env = EnvDiscrete\n",
    "\n",
    "    environment = env(\n",
    "        code=config['code'],\n",
    "        day=day,\n",
    "        latency=config['latency'],\n",
    "        T=config['time_window'],\n",
    "        # state ablation\n",
    "        wo_lob_state=config['wo_lob_state'],\n",
    "        wo_market_state=config['wo_market_state'],\n",
    "        # reward ablation\n",
    "        wo_dampened_pnl=config['wo_dampened_pnl'],\n",
    "        wo_matched_pnl=config['wo_matched_pnl'],\n",
    "        wo_inv_punish=config['wo_inv_punish'],\n",
    "        # exp setting\n",
    "        experiment_name=config['exp_name'],\n",
    "        log=config['log'],\n",
    "        )\n",
    "    return environment\n",
    "\n",
    "def init_agent(environment, config):\n",
    "    kwargs=dict()\n",
    "    if config['agent_type'] == 'dueling_dqn':\n",
    "        get_agent = get_dueling_dqn_agent\n",
    "        kwargs['learning_rate']=config['learning_rate']\n",
    "        kwargs['horizon']=config['horizon']\n",
    "    elif config['agent_type'] == 'ppo':\n",
    "        get_agent = get_ppo_agent\n",
    "        kwargs['learning_rate']=config['learning_rate']\n",
    "        kwargs['horizon']=config['horizon']\n",
    "\n",
    "    if config['wo_pretrain']:\n",
    "        print(\"Ablation: pretrain\")\n",
    "        lob_model = get_lob_model(64,config['time_window'])\n",
    "        lob_model.compute_output_shape = compute_output_shape\n",
    "    else:\n",
    "        pretrain_model_dir = f'./model_tensorflow2'\n",
    "        model = get_lob_model(64,config['time_window'])\n",
    "        model.compute_output_shape = compute_output_shape\n",
    "        model_pretrain = get_pretrain_model(model,config['time_window'])\n",
    "        checkpoint_filepath = pretrain_model_dir + '/china_model.weights.all.h5'\n",
    "        model_pretrain.load_weights(checkpoint_filepath)\n",
    "        lob_model = model_pretrain.layers[1]\n",
    "\n",
    "    if config['wo_attnlob']:\n",
    "        print(\"Ablation: attnlob\")\n",
    "        lob_model = get_fclob_model(64,config['time_window'])\n",
    "\n",
    "    model = get_model(\n",
    "        lob_model,\n",
    "        config['time_window'],\n",
    "        with_lob_state= not config['wo_lob_state'],\n",
    "        with_market_state= not config['wo_market_state']\n",
    "        )\n",
    "    agent = get_agent(model, environment=environment, max_episode_timesteps=1000, device=config['device'], **kwargs)\n",
    "\n",
    "    if config['load']:\n",
    "        model = keras.models.load_model(keras_model_dir)\n",
    "        model.layers[1].compute_output_shape = compute_output_shape\n",
    "        agent = get_agent(model, environment=environment, max_episode_timesteps=1000, device=config['device'], **kwargs)\n",
    "        agent.restore(config['agent_load_dir'], filename='cppo', format='numpy')\n",
    "\n",
    "    return agent\n",
    "\n",
    "def train_a_day(environment, agent, train_result):\n",
    "    num_episodes = len(environment.orderbook)//num_step_per_episode\n",
    "    data_collector = list()\n",
    "    for idx in tqdm(range(num_episodes)):\n",
    "        episode_states = list()\n",
    "        episode_actions = list()\n",
    "        episode_terminal = list()\n",
    "        episode_reward = list()\n",
    "\n",
    "        states = environment.reset_seq(timesteps_per_episode=num_step_per_episode, episode_idx=idx)\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            episode_states.append(states)\n",
    "            actions = agent.act(states=states, independent=True)\n",
    "            episode_actions.append(actions)\n",
    "            states, terminal, reward = environment.execute(actions=actions)\n",
    "            episode_terminal.append(terminal)\n",
    "            episode_reward.append(reward)\n",
    "\n",
    "        data_collector.append([episode_states, episode_actions, episode_terminal, episode_reward])\n",
    "\n",
    "        agent.experience(\n",
    "            states=episode_states,\n",
    "            actions=episode_actions,\n",
    "            terminal=episode_terminal,\n",
    "            reward=episode_reward\n",
    "        )\n",
    "\n",
    "        agent.update()\n",
    "\n",
    "        save_episode_result(environment, train_result)\n",
    "\n",
    "    return episode_states, episode_actions, episode_reward\n",
    "\n",
    "def test_a_day(environment, agent, test_result):\n",
    "    num_episodes = len(environment.orderbook)//num_step_per_episode\n",
    "    for idx in tqdm(range(num_episodes)):\n",
    "\n",
    "        states = environment.reset_seq(timesteps_per_episode=num_step_per_episode, episode_idx=idx)\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            actions = agent.act(\n",
    "                states=states, independent=True\n",
    "            )\n",
    "            states, terminal, reward = environment.execute(actions=actions)\n",
    "\n",
    "        save_episode_result(environment, test_result)\n",
    "\n",
    "def train(agent, train_result, config):\n",
    "    for day in train_days:\n",
    "        environment = init_env(day, config)\n",
    "        train_a_day(environment, agent, train_result)\n",
    "\n",
    "def test(agent, test_result, config):\n",
    "    for day in test_days:\n",
    "        environment = init_env(day, config)\n",
    "        test_a_day(environment, agent, test_result)\n",
    "\n",
    "def save_episode_result(environment, test_result):\n",
    "    res_dict = environment.get_final_result()\n",
    "    date = environment.day\n",
    "    idx = environment.episode_idx\n",
    "\n",
    "    test_result.loc[date+'_'+str(idx)] = [res_dict['pnl'], res_dict['nd_pnl'], res_dict['avg_abs_position'], res_dict['profit_ratio'], res_dict['volume']]\n",
    "\n",
    "def gather_test_results(test_result):\n",
    "    day_list = list(test_result.index)\n",
    "    for i in range(len(day_list)):\n",
    "        day_list[i] = day_list[i][:10]\n",
    "    day_list = set(day_list)\n",
    "    gathered_results = pd.DataFrame(columns=['PnL', 'ND-PnL', 'average_position', 'profit_ratio', 'volume'])\n",
    "    for day in day_list:\n",
    "        result = test_result[test_result.index.str.contains(day)]\n",
    "        pnl = result.PnL.sum()\n",
    "        nd_pnl = result['ND-PnL'].sum()\n",
    "        ap = result.average_position.mean()\n",
    "        volume = (result.PnL/result.profit_ratio).sum()\n",
    "        pr = pnl/volume\n",
    "        gathered_results.loc[day] = [pnl,nd_pnl,ap,pr,volume]\n",
    "    gathered_results=gathered_results.sort_index()\n",
    "    return gathered_results\n",
    "\n",
    "def save_agent(agent, config):\n",
    "    # save agent network\n",
    "    agent.model.policy.network.keras_model.save(keras_model_dir)\n",
    "    # Save agent\n",
    "    agent.save(config['agent_save_dir'], filename=None, format='numpy')\n",
    "\n",
    "#@pyrallis.wrap()\n",
    "def main(config: TrainConfig):\n",
    "    config = asdict(config)\n",
    "\n",
    "    environment = init_env(train_days[0], config)\n",
    "    agent = init_agent(environment, config)\n",
    "\n",
    "    train_result = pd.DataFrame(columns=['PnL', 'ND-PnL', 'average_position', 'profit_ratio', 'volume'])\n",
    "    for _ in range(n_train_loop):\n",
    "        train(agent, train_result, config)\n",
    "        if config['save']:\n",
    "            save_agent(agent, config)\n",
    "\n",
    "    test_result = pd.DataFrame(columns=['PnL', 'ND-PnL', 'average_position', 'profit_ratio', 'volume'])\n",
    "    test(agent, test_result, config)\n",
    "    daily_test_results = gather_test_results(test_result)\n",
    "    print(daily_test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "411e0001-d239-4ca5-8edd-f679b4c953fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: Continuous\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1619/3202694661.py:288: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  self.is_trade.loc[set(self.trade.TradingTime)] = 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TensorforceError.type() got an unexpected keyword argument 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m config \u001b[38;5;241m=\u001b[39m TrainConfig()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 1454\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m   1451\u001b[0m config \u001b[38;5;241m=\u001b[39m asdict(config)\n\u001b[1;32m   1453\u001b[0m environment \u001b[38;5;241m=\u001b[39m init_env(train_days[\u001b[38;5;241m0\u001b[39m], config)\n\u001b[0;32m-> 1454\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43minit_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1456\u001b[0m train_result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPnL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mND-PnL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_position\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprofit_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvolume\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_train_loop):\n",
      "Cell \u001b[0;32mIn[8], line 1351\u001b[0m, in \u001b[0;36minit_agent\u001b[0;34m(environment, config)\u001b[0m\n\u001b[1;32m   1343\u001b[0m     lob_model \u001b[38;5;241m=\u001b[39m get_fclob_model(\u001b[38;5;241m64\u001b[39m,config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_window\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m   1345\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(\n\u001b[1;32m   1346\u001b[0m     lob_model,\n\u001b[1;32m   1347\u001b[0m     config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_window\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1348\u001b[0m     with_lob_state\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwo_lob_state\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1349\u001b[0m     with_market_state\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwo_market_state\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m   1350\u001b[0m     )\n\u001b[0;32m-> 1351\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mget_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episode_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m   1354\u001b[0m     model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(keras_model_dir)\n",
      "Cell \u001b[0;32mIn[8], line 1091\u001b[0m, in \u001b[0;36mget_ppo_agent\u001b[0;34m(network, environment, states, actions, max_episode_timesteps, batch_size, learning_rate, horizon, discount, device)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_ppo_agent\u001b[39m(\n\u001b[1;32m   1079\u001b[0m                 network,\n\u001b[1;32m   1080\u001b[0m                 environment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1088\u001b[0m                 device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1089\u001b[0m                 ):\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m environment \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1091\u001b[0m         agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m            \u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mppo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m            \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_episode_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_episode_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdiscount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscount\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_interactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1103\u001b[0m         agent \u001b[38;5;241m=\u001b[39m Agent\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m   1104\u001b[0m             agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1105\u001b[0m             environment\u001b[38;5;241m=\u001b[39menvironment,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             parallel_interactions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m   1115\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorforce/agents/agent.py:116\u001b[0m, in \u001b[0;36mAgent.create\u001b[0;34m(agent, environment, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m tensorforce\u001b[38;5;241m.\u001b[39magents\u001b[38;5;241m.\u001b[39magents:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# Keyword specification\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     agent \u001b[38;5;241m=\u001b[39m tensorforce\u001b[38;5;241m.\u001b[39magents\u001b[38;5;241m.\u001b[39magents[agent]\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TensorforceError\u001b[38;5;241m.\u001b[39mvalue(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAgent.create\u001b[39m\u001b[38;5;124m'\u001b[39m, argument\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39magent)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorforce/agents/agent.py:87\u001b[0m, in \u001b[0;36mAgent.create\u001b[0;34m(agent, environment, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m         kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_episode_timesteps\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mmax_episode_timesteps()\n\u001b[0;32m---> 87\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(agent, Agent)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Agent\u001b[38;5;241m.\u001b[39mcreate(agent\u001b[38;5;241m=\u001b[39magent, environment\u001b[38;5;241m=\u001b[39menvironment)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorforce/agents/ppo.py:281\u001b[0m, in \u001b[0;36mProximalPolicyOptimization.__init__\u001b[0;34m(self, states, actions, max_episode_timesteps, batch_size, network, use_beta_distribution, memory, update_frequency, learning_rate, subsampling_fraction, optimization_steps, likelihood_ratio_clipping, discount, estimate_terminal, critic_network, critic_optimizer, preprocessing, exploration, variable_noise, l2_regularization, entropy_regularization, name, device, parallel_interactions, seed, execution, saver, summarizer, recorder, config)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m critic_optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    279\u001b[0m     baseline_objective \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 281\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Agent\u001b[39;49;00m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episode_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_episode_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_interactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_interactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_observe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecorder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecorder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Model\u001b[39;49;00m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msaver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msaver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummarizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexploration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexploration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariable_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariable_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml2_regularization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_regularization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TensorforceModel\u001b[39;49;00m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreward_estimation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_estimation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbaseline_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcritic_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_objective\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_objective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentropy_regularization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentropy_regularization\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorforce/agents/tensorforce.py:293\u001b[0m, in \u001b[0;36mTensorforceAgent.__init__\u001b[0;34m(self, states, actions, update, objective, reward_estimation, max_episode_timesteps, policy, memory, optimizer, baseline_policy, baseline_optimizer, baseline_objective, preprocessing, exploration, variable_noise, l2_regularization, entropy_regularization, name, device, parallel_interactions, buffer_observe, seed, execution, saver, summarizer, recorder, config)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buffer_observe \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parallel_interactions \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m summarizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     buffer_observe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episode_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_episode_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_interactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_interactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_observe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffer_observe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecorder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecorder\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(update, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    300\u001b[0m     update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimesteps\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mupdate)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorforce/agents/agent.py:212\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self, states, actions, max_episode_timesteps, parallel_interactions, buffer_observe, seed, recorder)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# States/actions specification\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates_spec \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mvalid_values_spec(\n\u001b[1;32m    210\u001b[0m     values_spec\u001b[38;5;241m=\u001b[39mstates, value_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m, return_normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    211\u001b[0m )\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactions_spec \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_values_spec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_normalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    214\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_episode_timesteps \u001b[38;5;241m=\u001b[39m max_episode_timesteps\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Check for name overlap\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorforce/util.py:385\u001b[0m, in \u001b[0;36mvalid_values_spec\u001b[0;34m(values_spec, value_type, return_normalized)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TensorforceError\u001b[38;5;241m.\u001b[39mvalue(\n\u001b[1;32m    381\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutil.valid_values_spec\u001b[39m\u001b[38;5;124m'\u001b[39m, argument\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue_type\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39mvalue_type\n\u001b[1;32m    382\u001b[0m     )\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_atomic_values_spec(values_spec\u001b[38;5;241m=\u001b[39mvalues_spec):\n\u001b[0;32m--> 385\u001b[0m     value_spec \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_value_spec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_normalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_normalized\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OrderedDict([(value_type, value_spec)])\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_normalized:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorforce/util.py:526\u001b[0m, in \u001b[0;36mvalid_value_spec\u001b[0;34m(value_spec, value_type, accept_underspecified, return_normalized)\u001b[0m\n\u001b[1;32m    524\u001b[0m     max_value \u001b[38;5;241m=\u001b[39m max_value\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(min_value, \u001b[38;5;28mfloat\u001b[39m):\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mTensorforceError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margument\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin_value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(max_value, \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m    528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TensorforceError\u001b[38;5;241m.\u001b[39mtype(name\u001b[38;5;241m=\u001b[39mvalue_type, argument\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_value\u001b[39m\u001b[38;5;124m'\u001b[39m, value\u001b[38;5;241m=\u001b[39mmax_value)\n",
      "\u001b[0;31mTypeError\u001b[0m: TensorforceError.type() got an unexpected keyword argument 'value'"
     ]
    }
   ],
   "source": [
    "resample = False\n",
    "if resample == False:\n",
    "    order_file='order.csv'\n",
    "    trade_file='trade.csv'\n",
    "    price_file='price.csv'\n",
    "    msg_file='msg.csv'\n",
    "    order_book_file='order_book.csv'\n",
    "else:\n",
    "    order_file='order_sample.csv'\n",
    "    trade_file='trade_sample.csv'\n",
    "    price_file='price_sample.csv'\n",
    "    msg_file='msg_sample.csv'\n",
    "    order_book_file='order_book_sample.csv'\n",
    "\n",
    "keras_model_dir='model'\n",
    "train_days=['20230320']\n",
    "#train_days=['20230320']\n",
    "test_days=['20230322']\n",
    "num_step_per_episode = 2000\n",
    "n_train_loop = 5\n",
    "#\n",
    "config = TrainConfig()\n",
    "main(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8868796-f435-4223-8863-869b8fd3c245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
